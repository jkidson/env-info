---
title: "Sarah McCutcheon Env-Info. Lab 2 & 3"
output: 
  html_document:
    toc: true
    toc_depth: 3
---

```{r}
# set working directory if has child directory
dir_child = 'students' 
if (dir_child %in% list.files()){
  if (interactive()) {  
    # R Console
    setwd(dir_child)
  } else {              
    # knitting
    knitr::opts_knit$set(root.dir=dir_child)  
  }
}
```

##Lab 2, HW 1


####Organization
[fire-mitigation](https://github.com/fire-mitigation)

### Content

```      
What is your burning environmental question that you would like to address? Feel free to provide group project, dissertation, and/or personal interest. What is the study area?
```


I would like to determine what California cities are most like Santa Barbara in regards to wildfire potential. I would like to create a weighted model, from expert opinion, to select cities based on the weighted factors. 

### Techniques
        
```
What techniques from the course do you think will be most applicable?
```

  I think the lessons on interactive plots and maps will be most useful to my specific question. I also believe, that the wrangling data and visualizing data lessons will be useful overall to my future career. 
        
#### Data
        
```
What data have you already identified? Feel free to provide a link and/or details on the variables of interest.
```

  
  The data that we will use for our weighted model will come from two sources. One will be the various data layers that we have for GIS (e.g. vegetation, wildland urban interface, census, fire history, and jurisdictional layers). Our second data source will be a collective weight that is assigned to each of those variables based on the expert opinion. We are collecting that information on Thursday, January 21st from members of the Santa Barbara Fire Safe Council using the [analytical hierarchy process](https://en.wikipedia.org/wiki/Analytic_hierarchy_process).


####Our Process

1. Collect expert opinion on weighted factors using AHP survey
2. Collect and organize necessary map layers    
    + Get GIS layers
    + Clean them up and organize attribute table to show only the data that is needed
3. Combine the weighted factors and the layers in a GIS model to select the appropriate cities

  
###Image

```
2010 WUI Map
```
A map of the _most recent_ comprehensive calculation of total wildland urban intermix in the US is shown below. The map was created by the __US Forest Service__.

![](images/smccutcheon_2010_WUImap.png)

###Data File
```{r}
#read csv
d=read.csv('data/smccutcheon_FullData.csv')

#output summary
summary(d)
```


##Lab 3, HW 2


###Data Wrangling
```{r, eval=FALSE}

# list of packages
pkgs = c(
  'readr',        # read csv
  'readxl',       # read xls
  'dplyr',        # data frame manipulation
  'tidyr',        # data tidying
  'nycflights13', # test dataset of NYC flights for 2013
  'gapminder')    # test dataset of life expectancy and popultion

# install packages if not found
for (p in pkgs){
  if (!require(p, character.only=T)){
    install.packages(p)
  }
}

```

####Traditional csv read
```{r}

d = read.csv('../data/r-ecology/species.csv')
d
head(d)
summary(d)

```

####Using readr to read csv
```{r}

library(readr)

d = read_csv('../data/r-ecology/species.csv')
d
head(d)
summary(d)
```

#### Code using basic functions only
```{r}
#read in csv
surveys = read.csv('../data/r-ecology/surveys.csv')

# view data
head (surveys)
summary (surveys)

# limit columns to species and year
surveys_2 = surveys[,c('species_id', 'year')]

# limit rows to just species "NL"
surveys_3 = surveys_2[,c('species_id', 'year')]

# get count per year
surveys_4 = aggregate(species_id ~ year, data=surveys_3, FUN= 'length')

# write to csv
write.csv(surveys_4, 'data/surveys_smccutcheon.csv', row.names = FALSE)
```
####Code using nested functions
```{r}
# read in data
surveys = read.csv('../data/r-ecology/surveys.csv')

# view data
head(surveys)
summary(surveys)

# limit data with [], aggregate to count, write to csv
write.csv(
  aggregate(
    species_id ~ year, 
    data = surveys[surveys_2$species_id  == 'NL', c('species_id', 'year')], 
    FUN = 'length'), 
  'data/surveys_smccutcheon.csv',
  row.names = FALSE)
```
####Code using dplyr
```{r, message = FALSE}
# load libraries
library(readr)
library(dplyr)

# read in csv
surveys = read_csv('../data/r-ecology/surveys.csv') 

# dplyr elegance
surveys %T>%                          # note tee operator %T>% for glimpse
  glimpse() %>%                       # view data
  select(species_id, year) %>%        # limit columns
  filter(species_id  == 'NL') %>%     # limit rows
  group_by(year) %>%                  # get count by first grouping
  summarize(n = n()) %>%              #   then summarize
  write_csv('data/surveys_smccutcheon.csv') # write out csv
```






## Week 4, Tidy Data

#### data

#### EDAWR

```{r EDAWR, eval=F}
# install.packages("devtools")
#devtools::install_github("rstudio/EDAWR")
library(EDAWR)
help(package='EDAWR')
?storms    # wind speed data for 6 hurricanes
?cases     # subset of WHO tuberculosis
?pollution # pollution data from WHO Ambient Air Pollution, 2014
?tb        # tuberculosis data
View(storms)
View(cases)
View(pollution)
```

### slicing

```{r traditional R slicing, eval=F}
# storms
storms$storm
storms$wind
storms$pressure
storms$date

# cases
cases$country
names(cases)[-1]
unlist(cases[1:3, 2:4])

# pollution
pollution$city[c(1,3,5)]
pollution$amount[c(1,3,5)]
pollution$amount[c(2,4,6)]

# ratio
storms$pressure / storms$wind
```

#### tidyr

Two main functions: gather() and spread() 

```{r tidyr, eval=F}
# install.packages("tidyr")
library(tidyr)
??gather # gather to long
??spread # spread to wide
```

### `gather`

```{r gather, eval=F}
cases
gather(cases, "year", "n", 2:4)

cases

library(dplyr)
d2<-cases %>%
  gather("year", "n", -country) %>%
  filter(
    year %in% c(2011, 2013) &
      !country %in% c('FR', 'US'))
d2
 
```

### `spread`

```{r spread, eval=F}
pollution
spread(pollution, size, amount)
```

Other functions to extract and combine columns...

### `separate`

```{r separate, eval=F}
storms
storms2 = separate(storms, date, c("year", "month", "day"), sep = "-")
```

### `unite`

```{r unite, eval=F}
storms2
unite(storms2, "date", year, month, day, sep = "-")
```



#### tidy CO<sub>2</sub> emissions

####**Task**. Convert the following table [CO<sub>2</sub> emissions per country since 1970](http://edgar.jrc.ec.europa.eu/overview.php?v=CO2ts1990-2014&sort=des9) from wide to long format and output the first few rows into your Rmarkdown. I recommend consulting `?gather` and you should have 3 columns in your output._

```{r read co2, eval=F}
library(readxl) # install.packages('readxl')

url = 'http://edgar.jrc.ec.europa.eu/news_docs/CO2_1970-2014_dataset_of_CO2_report_2015.xls'
xls = '../data/co2_europa.xls'

print(getwd())
if (!file.exists(xls)){
  download.file(url, xls)
}
co2 = read_excel(xls, skip=12)
co2
```

_**Question**. Why use `skip=12` argument in `read_excel()`?_

#### dplyr

A package that helps transform tabular data

```{r dplyr, eval=F}
# install.packages("dplyr")
library(dplyr)
?select
?filter
?arrange
?mutate
?group_by
?summarise
```

See sections in the [data-wrangling-cheatsheet.pdf](./refs/cheatsheets/data-wrangling-cheatsheet.pdf):

- Subset Variables (Columns), eg `select()`
- Subset Observations (Rows), eg `filter()`
- Reshaping Data - Change the layout of a data set, eg `arrange()`
- Make New Variables, eg `mutate()`
- Group Data, eg `group_by()` and `summarise()`

#### `select`

```{r select, eval=F}
storms
select(storms, storm, pressure)
storms %>% select(storm, pressure)
```

#### `filter`

```{r filter, eval=F}
storms
filter(storms, wind >= 50)
storms %>% filter(wind >= 50)

storms %>%
  filter(wind >= 50) %>%
  select(storm, pressure)
```

#### `mutate`

```{r mutate, eval=F}
storms %>%
  mutate(ratio = pressure / wind) %>%
  select(storm, ratio)
```

#### `group_by`

```{r group_by, eval=F}
pollution
pollution %>% group_by(city)
```

#### `summarise`

```{r summarise, eval=F}
# by city
pollution %>% 
  group_by(city) %>%
  summarise(
    mean = mean(amount), 
    sum = sum(amount), 
    n = n())

# by size
pollution %>% 
  group_by(size) %>%
  summarise(
    mean = mean(amount), 
    sum = sum(amount), 
    n = n())
```

note that `summarize` synonymously works

#### `ungroup`

```{r ungroup, eval=F}
pollution %>% 
  group_by(size)

pollution %>% 
  group_by(size) %>%
  ungroup()
```

#### multiple groups

```{r multiple groups, eval=F}
tb %>%
  group_by(country, year) %>%
  summarise(cases = sum(cases))
  summarise(cases = sum(cases))
```

**Recap: dplyr**:

- Extract columns with `select()` and rows with `filter()`

- Sort rows by column with `arrange()`

- Make new columns with `mutate()`

- Group rows by column with `group_by()` and `summarise()`


#### summarize CO<sub>2</sub> emissions

_**Task**. Report the top 5 emitting countries (not World or EU28) for 2014 using your long format table. (You may need to convert your year column from factor to numeric, eg `mutate(year = as.numeric(as.character(year)))`. As with most analyses, there are multiple ways to do this. I used the following functions: `filter`, `arrange`, `desc`, `head`)_. 

_**Task**. Summarize the total emissions by country  (not World or EU28) across years from your long format table and return the top 5 emitting countries. (As with most analyses, there are multiple ways to do this. I used the following functions: `filter`, `arrange`, `desc`, `head`)_. 


#### joining data

#### `bind_cols`

```{r bind_cols, eval=F}
y = data.frame(
  x1 = c('A','B','C'), 
  x2 = c( 1 , 2 , 3), 
  stringsAsFactors=F)
z = data.frame(
  x1 = c('B','C','D'), 
  x2 = c( 2 , 3 , 4), 
  stringsAsFactors=F)
y
z
bind_cols(y, z)
```

#### `bind_rows`

```{r bind_rows, eval=F}
y
z
bind_rows(y, z)
```

#### `union`

```{r union, eval=F}
y
z
union(y, z)
```

#### `intersect`

```{r intersect, eval=F}
y
z
intersect(y, z)
```

#### `setdiff`

```{r setdiff, eval=F}
y
z
setdiff(y, z)
```

#### `left_join`

```{r left_join, eval=F}
songs = data.frame(
  song = c('Across the Universe','Come Together', 'Hello, Goodbye', 'Peggy Sue'),
  name = c('John','John','Paul','Buddy'), 
  stringsAsFactors=F)
artists = data.frame(
  name = c('George','John','Paul','Ringo'),
  plays = c('sitar','guitar','bass','drums'), 
  stringsAsFactors=F)
left_join(songs, artists, by='name')
```

#### `inner_join`

```{r inner_join, eval=F}
inner_join(songs, artists, by = "name")
```

#### `semi_join`

```{r semi_join, eval=F}
semi_join(songs, artists, by = "name")
```

#### `anti_join`

```{r anti_join, eval=F}
anti_join(songs, artists, by = "name")
```

#### summarize per capita CO<sub>2</sub> emissions 

You'll join the [gapminder](https://github.com/jennybc/gapminder) datasets to get world population per country.

_**Task**. Report the top 5 emitting countries (not World or EU28) per capita using your long format table. (You may need to convert your year column from factor to numeric, eg `mutate(year = as.numeric(as.character(year)))`. As with most analyses, there are multiple ways to do this. I used the following functions: `filter`, `arrange`, `desc`, `head`)_. 

_**Task**. Summarize the total emissions by country  (not World or EU28) per capita across years from your long format table and return the top 5 emitting countries. (As with most analyses, there are multiple ways to do this. I used the following functions: `filter`, `arrange`, `desc`, `head`)_. 

```{r gapminder, eval=F}
library(gapminder) # install.packages('gapminder')
```


##Assignment 4

*Task. Report the top 5 emitting countries (not World or EU28) per capita using your long format table. (You may need to convert your year column from factor to numeric, eg mutate(year = as.numeric(as.character(year))). As with most analyses, there are multiple ways to do this. I used the following functions: filter, arrange, desc, head).

*Task. Summarize the total emissions by country (not World or EU28) per capita across years from your long format table and return the top 5 emitting countries. (As with most analyses, there are multiple ways to do this. I used the following functions: filter, arrange, desc, head).


```{r}
library(dplyr)
library(readxl) # install.packages('readxl')

url = 'http://edgar.jrc.ec.europa.eu/news_docs/CO2_1970-2014_dataset_of_CO2_report_2015.xls'
xls = '../data/co2_europa.xls'

print(getwd())
if (!file.exists(xls)){
  download.file(url, xls)
}
co2 = read_excel(xls, skip=12)
co2
```


####Top 6 Countries by CO2 Emissions

```{r}
library(dplyr)
library(tidyr)
co2long<-co2 %>%
  gather(Year, "Emissions",  -Country) %>%
  filter(
      !Country %in% c('World', 'EU28'))%>%
  mutate(Year = as.numeric(as.character(Year)))%>%
  group_by(Country)%>%
  summarise(
    TotalEmissions = sum(Emissions))%>%
  arrange(desc(TotalEmissions))
  
head(co2long)
```

